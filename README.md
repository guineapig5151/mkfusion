# MKFusion: Multi-Modal Knowledge Distillation for 4D Radar Point Cloud Segmentation in Autonomous Driving

This is the code for paper "[MKFusion: Multi-Modal Knowledge Distillation for 4D Radar Point Cloud Segmentation in Autonomous Driving](TBD)". 

MKFusion is a sparse multi-modal network that combines multi-scale radarâ€“camera fusion with knowledge distillation for 4D radar point cloud segmentation. 

## ðŸ”§ Install
You can prepare environment via conda:
```bash
conda env create -f env.yml -n mkfusion
```
Clone our repository and install (based on [MMDetection3D](https://github.com/open-mmlab/mmdetection3d)):
```bash
pip install -e . -v
```

Install [OpenPCDet](https://github.com/open-mmlab/OpenPCDet.git):
```bash
git clone https://github.com/open-mmlab/OpenPCDet.git
python setup develop
```

## ðŸ“š Dataset Preparation

### 1. Download Datasets
We use [VoD](https://github.com/tudelft-iv/view-of-delft-dataset) and [TJ4DRadSet](https://github.com/TJRadarLab/TJ4DRadSet) in our experiments. Please download the dataset and put them in `data`. We organize data as follows:
```bash
data
 |
 |-- vod
 |     |
 |     |-- radar_5frames
 |     |         |â€”- ImageSets
 |     |         |-- testing
 |     |         |-- training
 |     |               |
 |     |               |-- calib
 |     |               |-- image_2
 |     |               |-- label_2
 |     |               |-- pose
 |     |               |-- velodyne
 |     |-- lidar
 |           |-- the same as radar_5frames
 |--- tj4d
 |     |â€”â€” ImageSets
 |     |-- testing
 |     |-- training
 |              |-- calib
 |              |-- image_2
 |              |-- label_2
 |              |-- velodyne
```

### 2. Generate pickle info and reduced points (in FoV)
First, we should generate pickle info and reduced points in FoV, which is common process in `mmdetecton3d` and `OpenPCDet` codebases.
```bash
VoD dataset: python tools/create_data.py --dataset vod --root-path ./data/vod/radar_5frames --out-dir ./data/vod/radar_5frames
TJ4D dataset: python tools/create_data.py --dataset tj4d --root-path ./data/tj4d --out-dir ./data/tj4d
```
### 3. Generate Segmentation Labels (via detection labels)
As these datasets do not provide segmentation labels, we generate them via 3D detection labels:

VoD dataset: `python tools/create_seg_label.py`, label will save to `data/vod_seg_label`. Please adjust path in the file accordingly to generate both lidar and radar segmentation labels, where the process is the same.

TJ4D dataset: `python tools/create_seg_label_tj4d.py`, label will save to `data/tj4d_seg_label`.

### 4. Transform the lidar point cloud to the radar coordinate system
run `python tools/create_vod_lidar_inradar.py`, which will save results to `data/vod/lidar_in_radarcoord/training/lidar` and `data/vod/lidar_in_radarcoord/training/lidar_radar`. Please copy the pickle files generated by step 2 to `data/vod/lidar_in_radarcoord/training`, which will be used in teacher training.

*Notice*: TJ4D dataset does not provide lidar data.

### 5. (Optional) Generate Depth Map

Install [Metric3D-V2](https://github.com/YvanYin/Metric3D), then run inference on VoD dataset according to their demo. Please save depth map to `data/vod_depth_metric3dv2`.

## ðŸš€ Run

### Checkpoint
Please download [checkpoints](https://drive.google.com/file/d/1RmWiFnkqXU5I_pSOSso4E9NmBa0HHWGt/view?usp=sharing), and put them in `checkpoints`.

### Evaluation

#### VoD Dataset
```bash
teacher: python tools/test.py --config configs/mkfusion_teacher_vod.py --checkpoint checkpoints/mkfusion_teacher.pth

single-modal: python tools/test.py --config configs/mkfusion_single_vod.py --checkpoint checkpoints/mkfusion_single_vod.pth

multi-modal (w/o depth): python tools/test.py --config configs/mkfusion_multi_vod.py --checkpoint checkpoints/mkfusion_multi_vod.pth

multi-model (w/ depth): python tools/test.py --config configs/mkfusion_multi_dm_vod.py --checkpoint checkpoints/mkfusion_multi_dm_vod.pth
```

#### TJ4DRadSet Dataset
```bash
multi-modal: python tools/test.py --config configs/mkfusion_multi_tj4d.py --checkpoint checkpoints/mkfusion_multi_tj4d.pth
```

### Training
*Notice*: Due to the limited amount of data, normal jitter may occur in the training results

#### Teacher Training
```bash
python tools/train.py --config configs/mkfusion_teacher_vod.py
```

#### Single Modal Training
Please adjust `teacher_cp_file` in `configs/mkfusion_single.py`, accordingly. Then run:
```bash
python tools/train.py --config configs/mkfusion_single.py
```

#### Multi-Modal Training
Please adjust `teacher_cp_file` in `configs/mkfusion_single.py`, accordingly. Then run:
```bash
python tools/train.py --config configs/mkfusion_multi_vod.py
```
*Notice*: For the multi-modal model with depth information, we recommend fine-tuning on the model without depth.

## Acknowledgement
[MMDetection3D](https://github.com/open-mmlab/mmdetection3d), [OpenPCDet](https://github.com/open-mmlab/OpenPCDet.git), [Metric3D](https://github.com/YvanYin/Metric3D), [DepthAnything-v2](https://github.com/DepthAnything/Depth-Anything-V2), [MoGe](https://github.com/microsoft/MoGe), [VoD dataset](https://github.com/tudelft-iv/view-of-delft-dataset), [TJ4DRadSet dataset](https://github.com/TJRadarLab/TJ4DRadSet)
